{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5_woOOlm3Do"
      },
      "source": [
        "#Mounting Google Drive in Google Colab allows seamless access to cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7MsLX8DMRiG",
        "outputId": "b97f3a75-c055-4af6-ac73-480165175dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4g9sdFBnXlT"
      },
      "source": [
        "#The code extracts frames from a video file and saves them as images in a specified output folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAj000ewTTv9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def extract_frames(video_path, output_folder):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    # Create output folder if not exists\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Initialize time stamp and frame count\n",
        "    timestamp = 0\n",
        "    frame_count = 0\n",
        "\n",
        "    # Read until video is completed or 300 frames have been extracted\n",
        "    while cap.isOpened() and frame_count < 300:\n",
        "        # Set the capture to the specific time stamp\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, timestamp * 1000)\n",
        "\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Check if frame is read successfully\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save frame with timestamp as filename\n",
        "        frame_filename = os.path.join(output_folder, f\"{timestamp}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "        # Increment timestamp by 1 second\n",
        "        timestamp += 1\n",
        "\n",
        "        # Increment frame count\n",
        "        frame_count += 1\n",
        "\n",
        "    # Release video capture object\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Path to the input video file\n",
        "video_path = \"/content/drive/MyDrive/Internship_Task/input.mp4\"\n",
        "\n",
        "# Output folder to save frames\n",
        "output_folder = \"/content/drive/MyDrive/Internship_Task/output1\"\n",
        "# Extract frames from video\n",
        "extract_frames(video_path, output_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXw89-pgnkBg"
      },
      "source": [
        "#Text Detection from Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pTHtvmDXeHG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import os\n",
        "\n",
        "def extract_text_from_frame(frame_path, search_text=None):\n",
        "    # Read the frame\n",
        "    frame = cv2.imread(frame_path)\n",
        "\n",
        "    # Convert frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Use pytesseract to do OCR\n",
        "    text = pytesseract.image_to_string(gray)\n",
        "\n",
        "    # If search text is provided, check if it exists in the OCR output\n",
        "    if search_text and search_text.lower() in text.lower():\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def analyse_frames(frame_folder, search_text=None):\n",
        "    # Initialize dictionary to store timestamps\n",
        "    timestamp_map = {}\n",
        "\n",
        "    # Iterate over each frame file in the folder\n",
        "    for frame_file in os.listdir(frame_folder):\n",
        "        # Get timestamp from filename\n",
        "        filename = os.path.splitext(frame_file)[0]\n",
        "        timestamp = int(filename)\n",
        "\n",
        "        # Path to the frame image\n",
        "        frame_path = os.path.join(frame_folder, frame_file)\n",
        "\n",
        "        # Perform OCR on frame\n",
        "        text_found = extract_text_from_frame(frame_path, search_text)\n",
        "\n",
        "        # If search text is found, record timestamp\n",
        "        if text_found:\n",
        "            timestamp_map[timestamp] = frame_file\n",
        "\n",
        "    return timestamp_map\n",
        "\n",
        "# Directory containing the frames\n",
        "frame_folder = \"/content/drive/MyDrive/Internship_Task/output1\"\n",
        "\n",
        "# Specific text to search for in frames (None if not searching for any specific text)\n",
        "search_text = \"Bakerloo Line\"\n",
        "\n",
        "# Analyse frames and retrieve timestamps when search text appears\n",
        "timestamp_map = analyse_frames(frame_folder, search_text)\n",
        "\n",
        "# Display timestamps when search text appears\n",
        "if timestamp_map:\n",
        "    print(\"Timestamps when '{}' appears in the frames:\".format(search_text))\n",
        "    for ts, frame_file in timestamp_map.items():\n",
        "        print(\"Timestamp:\", ts, \"Frame File:\", frame_file)\n",
        "else:\n",
        "    print(\"Text not found in the frames.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qr3Y93afE74",
        "outputId": "e8f0a4a2-fb2b-4489-f025-d3261edf0497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.24.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.110.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.14.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.14.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.14.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.36.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract\n",
        "!pip install gradio\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeK8jKCK7lt6"
      },
      "source": [
        "https://pallyy.com/tools/image-caption-generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSHkO6gPHpiM",
        "outputId": "d4f116a2-0eda-4996-f1a9-1420d3517274"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, ViTFeatureExtractor, VisionEncoderDecoderModel\n",
        "\n",
        "device = 'cpu'\n",
        "encoder_checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "decoder_checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "model_checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(encoder_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_checkpoint).to(device)\n",
        "\n",
        "def predict(image, max_length=64, num_beams=4):\n",
        "    image = image.convert('RGB')\n",
        "    image = feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    clean_text = lambda x: x.replace('', '').split('\\n')[0]\n",
        "    caption_ids = model.generate(image, max_length=max_length)[0]\n",
        "    caption_text = clean_text(tokenizer.decode(caption_ids))\n",
        "    return caption_text\n",
        "\n",
        "def generate_captions_for_frames(frame_dir, output_file):\n",
        "    # List all files in the frame directory\n",
        "    frame_files = os.listdir(frame_dir)\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for frame_file in frame_files:\n",
        "            # Load each frame image\n",
        "            frame_path = os.path.join(frame_dir, frame_file)\n",
        "            frame_image = Image.open(frame_path)\n",
        "\n",
        "            # Generate caption for the frame\n",
        "            caption = predict(frame_image)\n",
        "\n",
        "            # Write caption along with filename to the output file\n",
        "            f.write(f\"{frame_file}: {caption}\\n\")\n",
        "\n",
        "# Directory containing the video frames\n",
        "frame_directory = \"/content/drive/MyDrive/Internship_Task/output1\"\n",
        "# Output file to store captions\n",
        "output_file = \"/content/drive/MyDrive/Internship_Task/op.txt\"\n",
        "\n",
        "# Generate captions for frames and store them in the output file\n",
        "generate_captions_for_frames(frame_directory, output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txtkn1uql5kG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ou-NmfndtNE"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-QaAgfDmH68"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bRLynujEfDvz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load GPT-2 XL model and tokenizer\n",
        "model_name = \"gpt2-xl\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Load captions from file\n",
        "def load_captions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        captions = file.readlines()\n",
        "    return captions\n",
        "\n",
        "# Generate story based on captions\n",
        "def generate_story(captions):\n",
        "    # Initialize an empty list to store generated story chunks\n",
        "    story_chunks = []\n",
        "\n",
        "    # Split captions into smaller chunks\n",
        "    chunk_size = 50  # Adjust this value as needed\n",
        "    for caption in captions:\n",
        "        # Split caption into smaller chunks\n",
        "        chunks = [caption[i:i+chunk_size] for i in range(0, len(caption), chunk_size)]\n",
        "\n",
        "        # Generate story chunk for each smaller chunk\n",
        "        for chunk in chunks:\n",
        "            # Tokenize and encode prompt\n",
        "            input_ids = tokenizer.encode(chunk, return_tensors=\"pt\")\n",
        "\n",
        "            # Generate story continuation using the model\n",
        "            output = model.generate(input_ids, max_length=500, temperature=0.7, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "            # Decode generated story chunk\n",
        "            story_chunk = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Add generated story chunk to the list\n",
        "            story_chunks.append(story_chunk)\n",
        "\n",
        "    # Concatenate story chunks into a single story\n",
        "    story = \" \".join(story_chunks)\n",
        "    return story\n",
        "\n",
        "# File paths\n",
        "captions_file = \"/content/drive/MyDrive/Internship_Task/op.txt\"\n",
        "\n",
        "# Load captions\n",
        "captions = load_captions(captions_file)\n",
        "\n",
        "# Generate story based on captions\n",
        "story = generate_story(captions)\n",
        "\n",
        "# Print or save the generated story\n",
        "print(story)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoWCR35kjBZU"
      },
      "source": [
        "#ALTERNATIVE METHOD - USING GEMINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSeqBa2_jKvJ"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je9dSObWjH1v"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Function to generate story based on captions using Gemini API\n",
        "def generate_story_with_gemini(captions, prompt):\n",
        "    # Initialize an empty list to store generated story chunks\n",
        "    story_chunks = []\n",
        "\n",
        "    # Add the prompt to the beginning of the story\n",
        "    story_chunks.append(prompt)\n",
        "\n",
        "    # Split captions into smaller chunks\n",
        "    chunk_size = 50  # Adjust this value as needed\n",
        "    for caption in captions:\n",
        "        # Split caption into smaller chunks\n",
        "        chunks = [caption[i:i+chunk_size] for i in range(0, len(caption), chunk_size)]\n",
        "\n",
        "        # Generate story chunk for each smaller chunk\n",
        "        for chunk in chunks:\n",
        "            # Generate story continuation using the Gemini API\n",
        "            response = requests.post(\n",
        "                \"https://api.gemini-openai.com/v1/complete\",\n",
        "                json={\"text\": chunk, \"model_id\": \"gemini-turing-002\", \"max_tokens\": 500, \"temperature\": 0.7}\n",
        "            )\n",
        "\n",
        "            # Extract generated text from the API response\n",
        "            story_chunk = response.json()[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "            # Add generated story chunk to the list\n",
        "            story_chunks.append(story_chunk)\n",
        "\n",
        "    # Concatenate story chunks into a single story\n",
        "    story = \" \".join(story_chunks)\n",
        "    return story\n",
        "\n",
        "# Function to load captions from a file\n",
        "def load_captions_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        captions = file.readlines()\n",
        "    return captions\n",
        "\n",
        "# File path to the captions file\n",
        "captions_file = \"/content/drive/MyDrive/Internship_Task/op.txt\"\n",
        "\n",
        "# Load captions from the file\n",
        "captions = load_captions_from_file(captions_file)\n",
        "\n",
        "# Craft the prompt\n",
        "prompt = \"\"\"\n",
        "Prompt:\n",
        "You are a master storyteller tasked with crafting a captivating narrative based on a collection of intriguing captions. These captions offer glimpses into various scenes, characters, and emotions. Your challenge is to weave these disparate elements into a cohesive and enchanting tale that transports the reader to a world of wonder and adventure. Let your imagination roam freely as you bring these fragments together, creating a story that unfolds with each word, drawing the reader deeper into its enchanting embrace.\n",
        "\"\"\"\n",
        "\n",
        "# Generate story based on captions using Gemini API\n",
        "story = generate_story_with_gemini(captions, prompt)\n",
        "\n",
        "# Print the generated story\n",
        "print(story)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGcIykYSl7N3"
      },
      "source": [
        "#FACIAL RECOGNITION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFS038RKofM0"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python dlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlmE43rNl8SY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "from collections import Counter\n",
        "\n",
        "# Function to detect faces in a frame using dlib\n",
        "def detect_faces_dlib(frame):\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    faces = detector(frame, 1)\n",
        "    return faces\n",
        "\n",
        "# Directory containing frames\n",
        "frames_dir = \"/content/drive/MyDrive/Internship_Task/output1\"\n",
        "\n",
        "# Initialize a Counter to count face occurrences\n",
        "face_counter = Counter()\n",
        "\n",
        "# Iterate over frames in the directory\n",
        "for filename in os.listdir(frames_dir):\n",
        "    if filename.endswith(\".jpg\"):  # Assuming frames are stored as JPEG images\n",
        "        filepath = os.path.join(frames_dir, filename)\n",
        "        frame = cv2.imread(filepath)\n",
        "        if frame is not None:\n",
        "            # Detect faces in the frame\n",
        "            faces = detect_faces_dlib(frame)\n",
        "            # Increment the count for each detected face\n",
        "            for face in faces:\n",
        "                face_counter[str(face)] += 1\n",
        "\n",
        "# Determine the most occurring face\n",
        "most_occurring_face, occurrences = face_counter.most_common(1)[0]\n",
        "\n",
        "print(\"Most occurring face:\", most_occurring_face)\n",
        "print(\"Occurrences:\", occurrences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZXj71Drm0JR"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}